{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# 初始化SparkSession\n",
    "from pyspark.sql.types import IntegerType, FloatType, DoubleType\n",
    "from code.temp import bins\n",
    "from webencodings import labels\n",
    "spark = SparkSession.builder.appName(\"PesticidesAnalysis\").getOrCreate()\n",
    "\n",
    "# 读取CSV文件到Spark DataFrame\n",
    "pesticides_path = \"E:/UOA/infosystem722/Assignment/a4/code/data/pesticides.csv\"\n",
    "pesticides = spark.read.csv(pesticides_path, header=True, inferSchema=True)\n",
    "\n",
    "# 显示DataFrame的结构\n",
    "pesticides.printSchema()\n",
    "\n",
    "# 显示DataFrame的前几行\n",
    "pesticides.show(5)\n",
    "\n",
    "# 显示DataFrame的描述性统计信息\n",
    "summary = pesticides.describe()\n",
    "summary.show()\n",
    "\n",
    "\n",
    "\n",
    "rainfall_path = \"E:/UOA/infosystem722/Assignment/a4/code/data/rainfall.csv\"\n",
    "rainfall = spark.read.csv(pesticides_path, header=True, inferSchema=True)\n",
    "rainfall.printSchema()\n",
    "\n",
    "rainfall.show(5)\n",
    "\n",
    "# 显示DataFrame的描述性统计信息\n",
    "summary = rainfall.describe()\n",
    "summary.show()\n",
    "\n",
    "\n",
    "temp_path = \"E:/UOA/infosystem722/Assignment/a4/code/data/rainfall.csv\"\n",
    "temp = spark.read.csv(temp_path, header=True, inferSchema=True)\n",
    "temp.printSchema()\n",
    "\n",
    "temp.show(5)\n",
    "\n",
    "# 显示DataFrame的描述性统计信息\n",
    "summary = temp.describe()\n",
    "summary.show()\n",
    "\n",
    "\n",
    "yield_path = \"E:/UOA/infosystem722/Assignment/a4/code/data/rainfall.csv\"\n",
    "yields = spark.read.csv(yield_path, header=True, inferSchema=True)\n",
    "yields.printSchema()\n",
    "\n",
    "yields.show(5)\n",
    "\n",
    "# 显示DataFrame的描述性统计信息\n",
    "summary = yields.describe()\n",
    "summary.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pesticides\n",
    "# 从Spark DataFrame提取数据列到NumPy数组\n",
    "values = np.array(pesticides.select(col('Value')).rdd.flatMap(lambda x: x).collect())\n",
    "Year = np.array(pesticides.select(col('Year')).rdd.flatMap(lambda x: x).collect())\n",
    "Domain = np.array(pesticides.select(col('Domain')).rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "# 设置风格\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 创建图形\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 绘制直方图\n",
    "sns.histplot(values, kde=True)  # 使用对数刻度和更多的分箱\n",
    "\n",
    "# 设置标题和标签\n",
    "plt.title('Distribution of Value')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 设置x轴的限制，根据你的数据调整这些值\n",
    "plt.ylim([1, 500])\n",
    "\n",
    "# 增加网格线\n",
    "plt.grid(True)\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=Year, y=values)\n",
    "plt.title('Scatter Plot of Year vs Value')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Value')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 设置风格\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 创建图形\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# 绘制条形图\n",
    "ax = sns.countplot(x=Year)\n",
    "\n",
    "# 设置标题和标签\n",
    "plt.title('Distribution of Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 优化x轴标签显示\n",
    "plt.xticks(rotation=45)  # 如果年份标签重叠，可以旋转标签\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=values)\n",
    "plt.title('Box Plot of Value')\n",
    "plt.xlabel('Value')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x=Domain)\n",
    "plt.title('Distribution of Domain Categories')\n",
    "plt.xlabel('Domain')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)  # 如果标签太长，可旋转以便更好的显示\n",
    "plt.show()\n",
    "\n",
    "# rainfall\n",
    "average_rain_fall_mm_per_year = np.array(rainfall.select(col('average_rain_fall_mm_per_year')).rdd.flatMap(lambda x: x).collect())\n",
    "Year = np.array(rainfall.select(col('Year')).rdd.flatMap(lambda x: x).collect())\n",
    "# 设置风格\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 创建图形\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 绘制直方图，对数刻度，调整bin大小\n",
    "sns.histplot(average_rain_fall_mm_per_year, kde=True)  # 使用对数刻度和更多的分箱\n",
    "\n",
    "# 设置标题和标签\n",
    "plt.title('Distribution of average_rain_fall_mm_per_year')\n",
    "plt.xlabel('average_rain_fall_mm_per_year')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 设置x轴的限制，根据你的数据调整这些值\n",
    "# plt.ylim([1, 500])\n",
    "\n",
    "# 增加网格线\n",
    "plt.grid(True)\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 设置风格\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 创建图形\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# 绘制条形图\n",
    "ax = sns.countplot(x=Year)\n",
    "\n",
    "# 设置标题和标签\n",
    "plt.title('Distribution of Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 优化x轴标签显示\n",
    "plt.xticks(rotation=45)  # 如果年份标签重叠，可以旋转标签\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n",
    "\n",
    "# temp\n",
    "Year = np.array(temp.select(col('year')).rdd.flatMap(lambda x: x).collect())\n",
    "avg_temp = np.array(temp.select(col('avg_temp')).rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "# 设置风格\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 创建图形\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 绘制直方图，对数刻度，调整bin大小\n",
    "sns.histplot(avg_temp, kde=True)  # 使用对数刻度和更多的分箱\n",
    "\n",
    "# 设置标题和标签\n",
    "plt.title('Distribution of avg_temp')\n",
    "plt.xlabel('avg_temp')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 设置x轴的限制，根据你的数据调整这些值\n",
    "# plt.ylim([1, 500])\n",
    "\n",
    "# 增加网格线\n",
    "plt.grid(True)\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 设置风格\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 创建图形\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# 绘制条形图\n",
    "ax = sns.countplot(x=Year)\n",
    "\n",
    "# 设置标题和标签\n",
    "plt.title('Distribution of Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 优化x轴标签显示\n",
    "plt.xticks(rotation=45)  # 如果年份标签重叠，可以旋转标签\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n",
    "\n",
    "# yields\n",
    "Value = np.array(yields.select(col('Value')).rdd.flatMap(lambda x: x).collect())\n",
    "avg_temp = np.array(yields.select(col('avg_temp')).rdd.flatMap(lambda x: x).collect())\n",
    "Year = np.array(yields.select(col('Year')).rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "# 设置风格\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 创建图形\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 绘制直方图，对数刻度，调整bin大小\n",
    "sns.histplot(Value, kde=True)  # 使用对数刻度和更多的分箱\n",
    "\n",
    "# 设置标题和标签\n",
    "plt.title('Distribution of Value')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 设置x轴的限制，根据你的数据调整这些值\n",
    "# plt.ylim([1, 500])\n",
    "\n",
    "# 增加网格线\n",
    "plt.grid(True)\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 设置风格\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 创建图形\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# 绘制条形图\n",
    "ax = sns.countplot(x=Year)\n",
    "\n",
    "# 设置标题和标签\n",
    "plt.title('Distribution of Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 优化x轴标签显示\n",
    "plt.xticks(rotation=45)  # 如果年份标签重叠，可以旋转标签\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 停止SparkSession\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, mean as _mean, stddev as _stddev, abs as _abs\n",
    "\n",
    "def z_score(df, column):\n",
    "    # 计算均值和标准差\n",
    "    stats = df.select(\n",
    "        _mean(col(column)).alias('mean'),\n",
    "        _stddev(col(column)).alias('stddev')\n",
    "    ).collect()[0]\n",
    "\n",
    "    mean = stats['mean']\n",
    "    std = stats['stddev']\n",
    "\n",
    "    # 计算Z-score并标记异常值\n",
    "    pesticides = df.withColumn('Z_score', _abs((col('Value') - mean) / std))\n",
    "    pesticides = df.withColumn('is_outlier', when(col('Z_score') > 3, 1).otherwise(0))\n",
    "\n",
    "    # 统计异常值的数量\n",
    "    outlier_count = pesticides.filter(col('is_outlier') == 1).count()\n",
    "\n",
    "    return outlier_count\n",
    "\n",
    "\n",
    "def describes(df):\n",
    "    print(df.printSchema())\n",
    "\n",
    "    # 打印DataFrame的描述性统计信息\n",
    "    print(df.describe().show())\n",
    "\n",
    "\n",
    "    # 打印DataFrame的行数和列数\n",
    "    num_rows = df.count()\n",
    "    num_cols = len(df.columns)\n",
    "    print(f\"DataFrame shape: ({num_rows}, {num_cols})\")\n",
    "\n",
    "    # 打印每列的缺失值数量\n",
    "    print(df.select([_mean(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show())\n",
    "\n",
    "\n",
    "describes(pesticides)\n",
    "# 打印结果\n",
    "print(\"Detected outliers using Z-score:\", z_score(pesticides, 'Value'))\n",
    "# 打印数据的基本信息\n",
    "\n",
    "\n",
    "\n",
    "describes(rainfall)\n",
    "print('average_rain_fall_mm_per_year ', \"Detected outliers using Z-score:\", z_score(rainfall, 'average_rain_fall_mm_per_year'))\n",
    "\n",
    "describes(temp)\n",
    "print(\"Detected outliers using Z-score:\", z_score(temp, 'avg_temp'))\n",
    "\n",
    "\n",
    "describes(yields)\n",
    "print(\"Detected outliers using Z-score:\", z_score(yields, 'Value'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 选择特定的列\n",
    "selected_pesticides = pesticides.select('Area', 'Year', 'Value')\n",
    "\n",
    "# 显示DataFrame的前几行\n",
    "selected_pesticides.show(5)\n",
    "\n",
    "# 选择特定的列\n",
    "selected_rainfall = rainfall.select(' Area', 'Year', 'average_rain_fall_mm_per_year')\n",
    "\n",
    "# 显示DataFrame的前几行\n",
    "selected_rainfall.show(5)# 选择特定的列\n",
    "\n",
    "selected_temp = temp.select('year', 'country', 'avg_temp')\n",
    "\n",
    "# 显示DataFrame的前几行\n",
    "selected_temp.show(5)# 选择特定的列\n",
    "\n",
    "selected_yields = pesticides.select('Area', 'Item', 'Year', 'Value')\n",
    "\n",
    "# 显示DataFrame的前几行\n",
    "selected_yields.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # 1. 删除空值\n",
    "    df = df.dropna()\n",
    "\n",
    "    # 2. 使用IQR去除离群值\n",
    "    for column in [col_name for col_name, dtype in df.dtypes if dtype in ['double', 'int']]:  # 只考虑数值型列\n",
    "        # 计算四分位数和IQR\n",
    "        quantiles = df.approxQuantile(column, [0.25, 0.75], 0.05)\n",
    "        Q1, Q3 = quantiles[0], quantiles[1]\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # 过滤离群值\n",
    "        df = df.filter((col(column) >= lower_bound) & (col(column) <= upper_bound))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# 调用函数\n",
    "print(selected_pesticides.show(5))\n",
    "cleaned_pesticides = clean_data(selected_pesticides)\n",
    "\n",
    "# 打印结果查看\n",
    "print(cleaned_pesticides.show(5))\n",
    "\n",
    "\n",
    "# 调用函数\n",
    "print(selected_temp.show(5))\n",
    "cleaned_temp = clean_data(selected_temp)\n",
    "\n",
    "# 打印结果查看\n",
    "print(cleaned_temp.show(5))\n",
    "\n",
    "\n",
    "\n",
    "# 调用函数\n",
    "print(selected_yields.show(5))\n",
    "cleaned_yields = clean_data(selected_yields)\n",
    "\n",
    "# 打印结果查看\n",
    "print(cleaned_yields.show(5))\n",
    "\n",
    "\n",
    "\n",
    "# 调用函数\n",
    "print(selected_rainfall.show(5))\n",
    "cleaned_rainfall = clean_data(selected_rainfall)\n",
    "\n",
    "# 打印结果查看\n",
    "print(cleaned_rainfall.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def merge_datasets(pesticides, temp, yields, rainfall):\n",
    "    # 重命名列以统一名称\n",
    "    pesticides = pesticides.withColumnRenamed('Value', 'pesticides_tonnes')\n",
    "    temp = temp.withColumnRenamed('country', 'Area').withColumnRenamed('year', 'Year')\n",
    "    yields = yields.withColumnRenamed('Value', 'hg_per_ha_yield')\n",
    "    rainfall = rainfall.withColumnRenamed(' Area', 'Area').withColumnRenamed('average_rain_fall_mm_per_year', 'avg_annual_rainfall_mm')\n",
    "\n",
    "    # 合并数据集，基于'Area'和'Year'\n",
    "    merged = pesticides.join(temp, on=['Area', 'Year'], how='outer')\n",
    "    merged = merged.join(yields, on=['Area', 'Year'], how='outer')\n",
    "    merged = merged.join(rainfall, on=['Area', 'Year'], how='outer')\n",
    "\n",
    "    return merged\n",
    "\n",
    "# 调用函数合并数据集\n",
    "merged_data = merge_datasets(cleaned_pesticides, cleaned_temp, cleaned_yields, cleaned_rainfall)\n",
    "\n",
    "# 显示合并后的数据\n",
    "print(merged_data.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "yield_df_path = \"E:/UOA/infosystem722/Assignment/a2/code/data/yield_df.csv\"\n",
    "yield_df = spark.read.csv(yield_df_path, header=True, inferSchema=True)\n",
    "describes(yield_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# yield_df\n",
    "Area = np.array(yield_df.select(col('Area')).rdd.flatMap(lambda x: x).collect())\n",
    "average_rain_fall_mm_per_year = np.array(yield_df.select(col('average_rain_fall_mm_per_year')).rdd.flatMap(lambda x: x).collect())\n",
    "pesticides_tonnes = np.array(yield_df.select(col('pesticides_tonnes')).rdd.flatMap(lambda x: x).collect())\n",
    "avg_temp = np.array(yield_df.select(col('avg_temp')).rdd.flatMap(lambda x: x).collect())\n",
    "hg_ha_yield = np.array(yield_df.select(col('hg/ha_yield')).rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "# 使用Seaborn的scatterplot函数绘制散点图\n",
    "sns.scatterplot(x=Area, y=hg_ha_yield)\n",
    "\n",
    "# 添加图表标题和轴标签\n",
    "plt.title('hg/ha_yield VS Area')\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('hg/ha_yield')\n",
    "\n",
    "# 旋转x轴标签，以便更清楚地显示\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 显示图表\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 绘制散点图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(average_rain_fall_mm_per_year, hg_ha_yield)\n",
    "\n",
    "\n",
    "# 添加标题和轴标签\n",
    "plt.title('average_rain_fall_mm_per_year vs. hg/ha_yield')\n",
    "plt.xlabel('average_rain_fall_mm_per_year')\n",
    "plt.ylabel('hg/ha_yield')\n",
    "\n",
    "# 显示图表\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 绘制散点图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pesticides_tonnes, hg_ha_yield)\n",
    "\n",
    "\n",
    "# 添加标题和轴标签\n",
    "plt.title('pesticides_tonnes vs. hg/ha_yield')\n",
    "plt.xlabel('pesticides_tonnes')\n",
    "plt.ylabel('hg/ha_yield')\n",
    "\n",
    "# 显示图表\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 绘制散点图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(avg_temp, hg_ha_yield)\n",
    "\n",
    "\n",
    "# 添加标题和轴标签\n",
    "plt.title('avg_temp vs. hg/ha_yield')\n",
    "plt.xlabel('avg_temp')\n",
    "plt.ylabel('hg/ha_yield')\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def reduce_data(df, target_column):\n",
    "    # 提取数值型列\n",
    "    numeric_columns = [field.name for field in df.schema.fields if field.dataType in [IntegerType(), FloatType(), DoubleType()]]\n",
    "\n",
    "    # 创建一个向量装配器\n",
    "    assembler = VectorAssembler(inputCols=numeric_columns, outputCol=\"features\")\n",
    "    df_vector = assembler.transform(df).select(\"features\")\n",
    "\n",
    "    # 计算相关矩阵\n",
    "    correlation_matrix = Correlation.corr(df_vector, \"features\").head()[0].toArray()\n",
    "\n",
    "    # 获取列名索引\n",
    "    col_index = {col: idx for idx, col in enumerate(numeric_columns)}\n",
    "\n",
    "    # 获取目标列的相关性\n",
    "    target_index = col_index[target_column]\n",
    "    target_corr = correlation_matrix[target_index]\n",
    "\n",
    "    # 筛选出与目标变量相关性较低的变量\n",
    "    low_corr_columns = [numeric_columns[i] for i, corr_value in enumerate(target_corr) if abs(corr_value) < 0.1]\n",
    "\n",
    "    # 打印哪些列将被移除\n",
    "    print(f\"Columns to be removed due to low correlation with target variable ('{target_column}'): {low_corr_columns}\")\n",
    "\n",
    "    # 从DataFrame中移除这些列\n",
    "    reduced_df = df.drop(*low_corr_columns)\n",
    "\n",
    "    return reduced_df\n",
    "\n",
    "# 调用函数并传入你的DataFrame以及目标列名称\n",
    "reduced_yield_df = reduce_data(yield_df, 'hg_per_ha_yield')\n",
    "\n",
    "# 显示简化后的数据的前几行\n",
    "print(reduced_yield_df.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# yield_df\n",
    "Area = np.array(reduced_yield_df.select(col('Area')).rdd.flatMap(lambda x: x).collect())\n",
    "average_rain_fall_mm_per_year = np.array(reduced_yield_df.select(col('average_rain_fall_mm_per_year')).rdd.flatMap(lambda x: x).collect())\n",
    "pesticides_tonnes = np.array(reduced_yield_df.select(col('pesticides_tonnes')).rdd.flatMap(lambda x: x).collect())\n",
    "avg_temp = np.array(reduced_yield_df.select(col('avg_temp')).rdd.flatMap(lambda x: x).collect())\n",
    "hg_ha_yield = np.array(reduced_yield_df.select(col('hg/ha_yield')).rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "\n",
    "# 绘制直方图\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(average_rain_fall_mm_per_year, kde=True)\n",
    "plt.title('Frequency of average_rain_fall_mm_per_year')\n",
    "plt.xlabel('average_rain_fall_mm_per_year')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# 使用when函数进行分箱\n",
    "reduced_yield_df = reduced_yield_df.withColumn(\n",
    "    \"rainfall_bins\",\n",
    "    when((col(\"average_rain_fall_mm_per_year\") > bins[0]) & (col(\"average_rain_fall_mm_per_year\") <= bins[1]), labels[0])\n",
    "    .when((col(\"average_rain_fall_mm_per_year\") > bins[1]) & (col(\"average_rain_fall_mm_per_year\") <= bins[2]), labels[1])\n",
    "    .when((col(\"average_rain_fall_mm_per_year\") > bins[2]) & (col(\"average_rain_fall_mm_per_year\") <= bins[3]), labels[2])\n",
    "    .when((col(\"average_rain_fall_mm_per_year\") > bins[3]) & (col(\"average_rain_fall_mm_per_year\") <= bins[4]), labels[3])\n",
    "    .when((col(\"average_rain_fall_mm_per_year\") > bins[4]) & (col(\"average_rain_fall_mm_per_year\") <= bins[5]), labels[4])\n",
    "    .when((col(\"average_rain_fall_mm_per_year\") > bins[5]) & (col(\"average_rain_fall_mm_per_year\") <= bins[6]), labels[5])\n",
    "    .otherwise(labels[6])\n",
    ")\n",
    "\n",
    "# 从DataFrame中移除指定列\n",
    "reduced_yield_df = reduced_yield_df.drop(\"average_rain_fall_mm_per_year\")\n",
    "\n",
    "# 显示分箱后的数据\n",
    "reduced_yield_df.show(5)\n",
    "\n",
    "\n",
    "# 绘制直方图\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(reduced_yield_df['rainfall_bins'])\n",
    "plt.title('Frequency of rainfall_bins')\n",
    "plt.xlabel('rainfall_bins')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, VectorIndexer\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "def train_random_forest(df, target_column, test_size=0.2, random_state=42):\n",
    "    # 删除NaN值\n",
    "    df = df.dropna()\n",
    "\n",
    "    # 检测并转换非数值型数据列\n",
    "    categorical_columns = [field.name for field in df.schema.fields if field.dataType == \"StringType\"]\n",
    "\n",
    "    indexers = [StringIndexer(inputCol=column, outputCol=column + \"_index\") for column in categorical_columns]\n",
    "    encoders = [OneHotEncoder(inputCol=column + \"_index\", outputCol=column + \"_encoded\") for column in categorical_columns]\n",
    "\n",
    "    # 将分箱结果转换为数值\n",
    "    if 'rainfall_bins' in df.columns:\n",
    "        indexer = StringIndexer(inputCol='rainfall_bins', outputCol='rainfall_bins_index')\n",
    "        df = indexer.fit(df).transform(df)\n",
    "\n",
    "    # 将所有特征列合并为一个向量列\n",
    "    feature_columns = [column + \"_encoded\" for column in categorical_columns if column in df.columns] + \\\n",
    "                      [col for col in df.columns if col not in categorical_columns and col != target_column]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "    # 分离出特征和目标变量\n",
    "    stages = indexers + encoders + [assembler]\n",
    "\n",
    "    # 创建随机森林回归模型\n",
    "    rf = RandomForestRegressor(featuresCol=\"features\", labelCol=target_column, numTrees=100, seed=random_state)\n",
    "\n",
    "    # 创建Pipeline\n",
    "    pipeline = Pipeline(stages=stages + [rf])\n",
    "\n",
    "    # 将数据分为训练集和测试集\n",
    "    train_df, test_df = df.randomSplit([1 - test_size, test_size], seed=random_state)\n",
    "\n",
    "    # 训练模型\n",
    "    model = pipeline.fit(train_df)\n",
    "\n",
    "    # 预测测试集\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    # 计算和打印性能指标\n",
    "    evaluator = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    mse = evaluator.evaluate(predictions)\n",
    "    r2_evaluator = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    r2 = r2_evaluator.evaluate(predictions)\n",
    "\n",
    "    print(\"Model Performance\")\n",
    "    print(\"Mean Squared Error (RMSE):\", mse)\n",
    "    print(\"R^2 Score:\", r2)\n",
    "\n",
    "    return model, test_df, predictions\n",
    "\n",
    "# 调用函数进行训练和评估\n",
    "model, X_test, y_test, y_pred = train_random_forest(reduced_yield_df, 'hg/ha_yield')\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.3)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)  # 绘制45度线\n",
    "plt.xlabel('Actual value')\n",
    "plt.ylabel('Predictive value')\n",
    "plt.title('Actual value vs Predictive value')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "errors = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(errors, bins=25)\n",
    "plt.xlabel('Prediction error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Prediction error distribution')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 提取特征重要性\n",
    "rf_model = model.stages[-1]  # RandomForestRegressor model is the last stage in the pipeline\n",
    "importances = rf_model.featureImportances.toArray()\n",
    "\n",
    "# 获取特征名称\n",
    "assembler = model.stages[-2]  # Assembler is the second last stage in the pipeline\n",
    "feature_names = assembler.getInputCols()\n",
    "\n",
    "# 转换为NumPy数组进行绘图\n",
    "indices = np.argsort(importances)[-10:]  # 获取最重要的10个特征的索引\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(indices)), importances[indices], align='center')\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "在PySpark中，确实没有直接提供与Scikit-learn中的SVR等效的支持向量回归模型。通常，在大数据处理环境中，Spark主要使用的是线性回归、决策树、随机森林等模型。\n",
    "你遇到的错误是因为LinearSVR（线性支持向量回归）目前在PySpark中还不支持。不过，你可以使用另一种回归模型，比如LinearRegression来替代支持向量回归。\n",
    "下面是如何在PySpark中使用LinearRegression来替代LinearSVR的代码：\n",
    "\"\"\"\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 初始化SparkSession\n",
    "spark = SparkSession.builder.appName(\"LinearRegression\").getOrCreate()\n",
    "\n",
    "# 假设你已经有了清洗后的Spark DataFrame：df\n",
    "# 读取CSV文件到Spark DataFrame (此处假设路径相同)\n",
    "data_path = \"path_to_your_data.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "def train_linear_regression(df, target_column, test_size=0.2, random_state=42):\n",
    "    # 删除NaN值\n",
    "    df = df.dropna()\n",
    "\n",
    "    # 检测并转换非数值型数据列\n",
    "    categorical_columns = [field.name for field in df.schema.fields if field.dataType == \"StringType\"]\n",
    "\n",
    "    indexers = [StringIndexer(inputCol=column, outputCol=column + \"_index\") for column in categorical_columns]\n",
    "    encoders = [OneHotEncoder(inputCol=column + \"_index\", outputCol=column + \"_encoded\") for column in categorical_columns]\n",
    "\n",
    "    # 将分箱结果转换为数值\n",
    "    if 'rainfall_bins' in df.columns:\n",
    "        indexer = StringIndexer(inputCol='rainfall_bins', outputCol='rainfall_bins_index')\n",
    "        df = indexer.fit(df).transform(df)\n",
    "\n",
    "    # 将所有特征列合并为一个向量列\n",
    "    feature_columns = [column + \"_encoded\" for column in categorical_columns if column in df.columns] + \\\n",
    "                      [col for col in df.columns if col not in categorical_columns and col != target_column]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "    # 标准化数据\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "    # 分离出特征和目标变量\n",
    "    stages = indexers + encoders + [assembler, scaler]\n",
    "\n",
    "    # 创建线性回归模型\n",
    "    lr = LinearRegression(featuresCol=\"scaled_features\", labelCol=target_column)\n",
    "\n",
    "    # 创建Pipeline\n",
    "    pipeline = Pipeline(stages=stages + [lr])\n",
    "\n",
    "    # 将数据分为训练集和测试集\n",
    "    train_df, test_df = df.randomSplit([1 - test_size, test_size], seed=random_state)\n",
    "\n",
    "    # 训练模型\n",
    "    model = pipeline.fit(train_df)\n",
    "\n",
    "    # 预测测试集\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    # 计算和打印性能指标\n",
    "    evaluator = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    mse = evaluator.evaluate(predictions)\n",
    "    r2_evaluator = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    r2 = r2_evaluator.evaluate(predictions)\n",
    "\n",
    "    print(\"Model Performance\")\n",
    "    print(\"Mean Squared Error (RMSE):\", mse)\n",
    "    print(\"R^2 Score:\", r2)\n",
    "\n",
    "    return model, train_df, test_df, predictions\n",
    "\n",
    "# 调用函数并传入你的DataFrame以及目标列名称\n",
    "model, train_df, test_df, predictions = train_linear_regression(df, 'hg_per_ha_yield')\n",
    "\n",
    "# 停止SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, StandardScaler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 初始化SparkSession\n",
    "spark = SparkSession.builder.appName(\"NeuralNetwork\").getOrCreate()\n",
    "\n",
    "# 假设你已经有了清洗后的Spark DataFrame：df\n",
    "# 读取CSV文件到Spark DataFrame (此处假设路径相同)\n",
    "data_path = \"path_to_your_data.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "def train_neural_network(df, target_column, test_size=0.2, random_state=42):\n",
    "    # 删除NaN值\n",
    "    df = df.dropna()\n",
    "\n",
    "    # 检测并转换非数值型数据列\n",
    "    categorical_columns = [field.name for field in df.schema.fields if field.dataType == \"StringType\"]\n",
    "\n",
    "    indexers = [StringIndexer(inputCol=column, outputCol=column + \"_index\") for column in categorical_columns]\n",
    "    encoders = [OneHotEncoder(inputCol=column + \"_index\", outputCol=column + \"_encoded\") for column in categorical_columns]\n",
    "\n",
    "    # 将分箱结果转换为数值\n",
    "    if 'rainfall_bins' in df.columns:\n",
    "        indexer = StringIndexer(inputCol='rainfall_bins', outputCol='rainfall_bins_index')\n",
    "        df = indexer.fit(df).transform(df)\n",
    "\n",
    "    # 将所有特征列合并为一个向量列\n",
    "    feature_columns = [column + \"_encoded\" for column in categorical_columns if column in df.columns] + \\\n",
    "                      [col for col in df.columns if col not in categorical_columns and col != target_column]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "    # 标准化数据\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "    # 分离出特征和目标变量\n",
    "    stages = indexers + encoders + [assembler, scaler]\n",
    "\n",
    "    # 创建Pipeline\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "\n",
    "    # 将数据分为训练集和测试集\n",
    "    train_df, test_df = df.randomSplit([1 - test_size, test_size], seed=random_state)\n",
    "\n",
    "    # 处理数据并进行转换\n",
    "    train_df = pipeline.fit(train_df).transform(train_df)\n",
    "    test_df = pipeline.fit(test_df).transform(test_df)\n",
    "\n",
    "    # 提取特征和标签\n",
    "    X_train = np.array(train_df.select(\"scaled_features\").rdd.map(lambda row: row[0]).collect())\n",
    "    y_train = np.array(train_df.select(target_column).rdd.map(lambda row: row[0]).collect())\n",
    "    X_test = np.array(test_df.select(\"scaled_features\").rdd.map(lambda row: row[0]).collect())\n",
    "    y_test = np.array(test_df.select(target_column).rdd.map(lambda row: row[0]).collect())\n",
    "\n",
    "    # 创建神经网络模型\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=X_train.shape[1]),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # 编译模型\n",
    "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "\n",
    "    # 训练模型\n",
    "    history = model.fit(X_train, y_train, epochs=100, validation_split=0.1, verbose=0)\n",
    "\n",
    "    # 预测测试集\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "    # 计算均方误差\n",
    "    mse = np.mean((y_test - y_pred)**2)\n",
    "    print(\"Model Performance\")\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "    return model, history, X_test, y_test, y_pred\n",
    "\n",
    "# 调用函数并传入你的DataFrame以及目标列名称\n",
    "_, _, _, y_test, y_pred = train_neural_network(df, 'hg_per_ha_yield')\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.3)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)  # 绘制45度线\n",
    "plt.xlabel('Actual value')\n",
    "plt.ylabel('Predictive value')\n",
    "plt.title('Actual value vs Predictive value')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "errors = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(errors, bins=25)\n",
    "plt.xlabel('Prediction error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Prediction error distribution')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
